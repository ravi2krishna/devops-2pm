19th Nov 2025
=============


    -> Version Control System - VCS 

        -> What ?

            -> Tracking Source Code 

            -> Store Application Code / Configuration

            -> Daily Changes From Development team will be placed in VCS 

        -> Why ?    

            -> Revert Changes 

                -> Any Application has multiple versions 

                    -> Why ?

                        -> Application will always evolve over time ?

                            -> WhatApps 

                                -> Messaging Platform (initial) - v1 - 2k files 
                                -> Audio Calls - v2 - 2.5k files 
                                -> Video Calls - v3 (2024 - ALL GOOD) - 4.5k files 
                                -> Payments - v4 ( 2025 ) (Issues will Calls ?) - 6.5k files 
                                -> etc 

                -> We have databases (Banking)

                    -> HDFC Bank 

                        -> creds
                        -> user profile
                        -> credits
                        -> debits 
                        -> transactions 

                    -> Which transaction
                    -> When transaction was made
                    -> What transaction type 
                    -> How much did we transact 

                -> We have an Project Called Notepad++ 

                    -> Want To Know 

                        -> What Changes Were Done in Last 1 Week    
                        -> What Changes Were Done in Last 1 Year
                        -> Who Contributed More 
                        -> What did they Contribute 

        -> NOTE: No VCS i,e No TRACKING HISTORY 

        -> In VSC Commits are unique identity numbers generated for 
            every change 

            -> commit id 40 digit hex code which stores following info 

                -> who made change 
                -> when they made change
                -> what change they made 

    -> Git      :   Client Software         [ Developer Utility]

    -> GitHub   :   Server Side Software    [ Organization ]

20th Nov 2025
=============

    -> Git Workflow 

    -> Git Repo 

    -> Git Commits 

    -> Git Push 

    -> Git Undo 

    -> Git Detach State 


21st Nov 2025
=============

    -> A Server is a computer system which provides resources to other computers known as clients 
        over internet / network 

    -> Server From Hardware 

        -> Processor 
        -> Memory (RAM)
        -> Storage (Hard Disk)
        -> Network (LAN)

    -> My Laptop == Server ? 

        -> My Laptop is server ? 

        -> A laptop can be used by one person at a time - Limited Hardware Configs 

        -> A Server can be used by multiple persons at a time - Higher Hardware Configs

        -> A laptop has GUI - Graphical User Interface  

        -> A server has CLI - Command User Interface 

    -> Server From Software  

        -> Server Softwares helps you provide services 

        -> Web Servers 

            -> Any website we see on internet needs Web Server to run 

            -> Web Serves helps you host Web Sites and Web Applications On WWW (internet)

            -> Examples : Nginx, Apache, IIS(windows) etc 

            -> HTTP / HTTPS - 80 / 443

        -> Email Servers 

            -> Email Serves helps you send and receive emails 

            -> Examples : sendmail, postfix, sendgrid etc 

            -> SMTP - 25 / 465 / 487

        -> Database Servers 

        -> Application Servers 

    -> Servers to be rented on Cloud Platforms 

        -> AWS, AZURE, GCP, OCI etc 

    -> Server On Azure 

        -> Server On Azure is called as Virtual Machine 

    -> Azure Virtual Machines (VM's) is a part of Azure's cloud-computing platform, 
        Microsoft, that "allows users to rent virtual computers(servers)" 
        on which to run their "own computer applications".

        -> Server                   ==> Azure Virtual Machine (VM)
        -> Server Name              ==> VM Name 
        -> Operating Systems        ==> Images ( Linux Ubuntu 22.04)
        -> CPU + RAM                ==> Sizes (b1s, b2s etc)           
        -> Login / Authentication   ==> Key Pairs (Public Key & Private Key)
        -> Network                  ==> VNET (Virtual Network)
        -> Firewall                 ==> Network Security Group (Rules: Protocol, Port, Source)
        -> Storage (Hard Disk)      ==> Disk is 30 GB 

    1  whoami
    2  logout
    3  ps -ef
    4  sudo netstat -ntpl
    5  sudo netstat
    6  cat /etc/os-release
    7  sudo ss -ntpl
    8  sudo systemctl stop ssh
    9  sudo ss -ntpl
   10  sudo systemctl start ssh
   11  sudo ss -ntpl
   12*
   13  sudo ss -ntpl
   14  sudo apt install nginx
   15  sudo ss -ntpl
   16  git
   17  ls /var/www/html/
   18  cat /var/www/html/index.nginx-debian.html
   19  git clone -b dev https://github.com/ravi2krishna/devops-2pm.git /var/www/html/
   20  cat /var/www/html/index.nginx-debian.html
   21  ls
   22  sudo rm /var/www/html/index.nginx-debian.html
   23  ls /var/www/html/
   24  git clone -b dev https://github.com/ravi2krishna/devops-2pm.git /var/www/html/
   25  sudo git clone -b dev https://github.com/ravi2krishna/devops-2pm.git /var/www/html/
   26  ls /var/www/html/
   27  ls /var/log/nginx/
   28  sudo cat /var/log/nginx/access.log
   29  ls /var/www/html/
   30  history

25th Nov 2025
=============

    -> Git Operation Using CLI 

        1  git clone https://github.com/ravi2krishna/devops-2pm.git
        2  ls
        3  cd devops-2pm
        4  git branch
        5  git checkout dev
        6  git branch
        7  git branch scripts
        8  git branch
        9  git checkout scripts
        10  git branch
        11  ls
        12  git status
        13  git config --global user.name devopsguy9
        14  git config --global user.email ravikrishna.vetagiri@gmail.com
        15  git status
        16  vi script.sh
        17  git status
        18  ls
        19  vi test.txt
        20  git status
        21  git add script.sh
        22  git status
        23  git restore --staged script.sh
        24  git status
        25  git restore --staged script.sh
        26  git status
        27  git commit -m "create script for web app deployment"
        28  git add script.sh
        29  git status
        30  git commit -m "create script for web app deployment"
        31  git status
        32  git restore test.txt
        33  git status
        34  git push
        35  git push --set-upstream origin scripts
        36  cd
        37  ls
        38  rm -rf devops-2pm/
        39  ls
        40  git clone -b scripts https://github.com/ravi2krishna/devops-2pm.git
        41  ls
        42  cd devops-2pm/
        43  ls
        44  sh script.sh


    -> Merge Conflicts 

        -> A Merge Conflict happens when two branches have been modified 
            with the changes and are subsequently merged.

        -> Git doesn’t know which changes to keep, and thus needs 
            human intervention(Manager) to resolve the conflict.

        -> When Automatic merge fails, we need to do Manual Merges

        45  history
   46  git branhc
   47  git branch
   48  git branch -a
   49  git branch b1
   50  git branch b2
   51  git branch
   52  git checkout b1
   53  git branch
   54  ls
   55  vi b1.txt
   56  git status
   57  git add b1.txt
   58  git commit -m "b1"
   59  ls
   60  git checkout b2
   61  ls
   62  vi b2.txt
   63  git status
   64  git add b2.txt
   65  git commit -m "b2:
   66  git commit -m "b2"
   67  ls
   68  git checkout scripts
   69  ls
   70  git merge b1
   71  ls
   72  git merge b2
   73  ls
   74  cat test.txt
   75  git branch
   76  git branch b3
   77  git branch b4
   78  git branch
   79  git checkout b3
   80  ls
   81  vi script.sh
   82  git add script.sh
   83  git checkout b3 done
   84  git commit -m "b3 done"
   85  git checkout b4
   86  ls
   87  vi script.sh
   88  git add script.sh
   89  git commit -m "b4 done"
   90  git checkout scripts
   91  cat script.sh
   92  git merge b3
   93  cat script.sh
   94  git merge b4
   95  cat script.sh
   96  git status
   97  vi script.sh
   98  git add script.sh
   99  git commit -m "Conflict resolved - noth aws & azure required"
  100  cat script.sh
  101  cd ~
  102  ls
  103  cd -
  104  history

  Rebase & Merge

    -> Merge 

        -> Maintains a clear history of how branches have diverged 
        and converged. Easier to understand the history of the project.

    -> Rebase 

        ->  Creates a cleaner, linear project history.

        105  cd ~
        106  ls
        107  git init proj_merge
        108  ls
        109  cd proj_merge/
        110  ls
        111  git branch
        112  git branch -a
        113  touch m1
        114  ls
        115  git status
        116  git branch -m main
        117  git status
        118  git add m1
        119  git commit -m "m1"
        120  git branch
        121  git status
        122  git branch feature
        123  git status
        124  git checkout -b feature
        125  git checkout feature
        126  touch f1
        127  git add f1
        128  git commit -m "f1"
        129  git checkout main
        130  ls
        131  touch m2
        132  git add m2
        133  git commit -m "m2"
        134  ls
        135  git merge feature
        136  git log --oneline
        137  cd ~
        138  ls
        139  git init proj_rebase
        140  cd proj_rebase/
        141  git status
        142  git branch -m main
        143  git status
        144  touch m1
        145  git add m1
        146  git commit -m "m1"
        147  git branch feature
        148  git checkout feature
        149  touch f1
        150  git add f1
        151  git commit -m "f1"
        152  git checkout main
        153  ls
        154  touch m2
        155  git add m2
        156  git commit -m "m2"
        157  git rebase feature
        158  git log --oneline
        159  git log --oneline --graph
        160  cd ~/proj_merge/
        161  git log --oneline --graph
        162  history

    -> Git Ignore 

        -> .gitignore file content is made of list of 
        files and directories to be excluded 

        163  ls
        164  vi .env
        165  git status
        166  vi .gitignore
        167  git status
        168  cat .env
        169  ls *.class
        170  touch file{1..10}.class
        171  ls
        172  git status
        173  cat .gitignore
        174  vi .gitignore
        175  git status
        176  vi .gitignore
        177  git status
        178  git add .gitignore
        179  git commit -m "ignore entry added"
        180  history
        181  sudo vi /etc/nginx/nginx.conf
        182  sudo vi /etc/nginx/sites-enabled/default
        183  history

26th Nov 2025
=============

    -> Applications will have an architecture 

    -> Three Tier Application Architecture is quite popular 
    for building web Applications 

    -> https://en.wikipedia.org/wiki/Programming_languages_used_in_most_popular_websites

    -> Their development typically involves server-side coding, 
        client-side coding and database technology.

    -> Our Application Goes with following technologies 

        -> Frontend : React 

        -> Backend : Node 

        -> Database : Postgres 

    -> Application Flow 

        -> User -> Frontend -> Backend -> Database 

    -> Software Builds 

        -> The term Build refers to a process by which 
            source code is converted into binary code

                -> Binary Code == Software Artifact 

    -> Any Build You Want To Perform Requires Below Entities 

        -> Developer Starts Writing Source Code 

            -> Source Code is Human Readable Instructions 
            -> Source Code is updated in GitHub (Repos)

        -> Developer Starts Writing Metadata Files 

            -> Metadata refers to Data about data 
            
            -> Metadata is information about the project 

                -> Project Name 
                -> Project Version 
                -> Project Dependencies 
                -> Project Test Suite 
                -> etc 

        -> NOTE: Source Code & Metadata is always part of repository 

    
    -> Build Artifacts will be produced after build is done 

        -> Build process is handled by Developer in Laptops 

        -> Build process is handled by DevOps Engineer in Server 

29th Nov 2025
=============

    -> Three Tier Application Architecture

    -> Install postgres (5432)

        -> sudo ss -ntpl

        -> Goto https://www.postgresql.org/download/linux/ubuntu/

        -> sudo ss -ntpl

    -> Every Database has connection properties, which includes
            Address Of Database
            Database Port
            Database Username
            Database Password

    -> Set Password - postgres 
        Commands to set the password for postgres database
            sudo su - postgres
            psql
            \password

    -> Setup Backend / App Layer For LMS 
        install Node.js version 16 on your system

            # Open New Session with ubuntu user
            whoami
            node -v
            npm -v
            curl -sL https://deb.nodesource.com/setup_16.x | sudo bash -
            sudo apt-get install -y nodejs
            node -v
            npm -v

    -> Build Backend For LMS
        
        Clone the code and Change the directory to api, 
        Then create a .env file in the api folder which is our backend source code,
        where we would specify the database connection details.

        # In ubuntu user Session
            ls
            git clone -b dev https://github.com/ravi2krishna/lms.git
            ls
            # Backend Source code
            cd ~/lms/api
            ls
            vi .env

            MODE=dev
            PORT=8080
            DATABASE_URL=postgresql://postgres:your-password@localhost:5432/postgres

            # ubuntu user Session
            cat prisma/migrations/20221110085013_init/migration.sql

            # Old Session  - postgres user
            sudo su - postgres
            psql
            \dt

            # ubuntu user Session
            # Install Dependencies 
            cd ~/lms/api
            npm install
            sudo npx prisma generate && sudo npx prisma db push

            # Old Session  - postgres user - Verify
            \dt

            # ubuntu user Session
            ls build
            # Build LMS API
            npm run build
            ls build

            # ubuntu user Session
            # Run/Start LMS API
            node build/index.js

            # Open New Session Again - ubuntu user - To Verify Port
            sudo ss -ntpl

            # New Session - ubuntu user
            ctrl + c
            pm2
            sudo npm install -g pm2
            pm2 start build/index.js
            sudo ss -ntpl

            Locally verify 
            curl http://localhost:8080/api


            {"message":"success","mode":"dev"}

        -> Verify with Browse - http://public-ip:8080/api
    
        -> Build Frontend For LMS
            
            -> Change the directory to webapp, 
            Then create a .env file in the webapp folder which is our frontend 
            source code, where we would specify the backend connection details.

            # Session - ubuntu user
            cd ~/lms/webapp
            ls
            vi .env

            VITE_API_URL=http://public-ip:8080/api

            -> Run the below commands to build the Application, 
            which will create dist folder, in which our application will 
                be made available

            npm install
            ls dist
            npm run build
            ls dist

        -> Setup Nginx Web Server

            sudo ss -ntpl
            sudo apt -y install nginx
            sudo ss -ntpl
            ls /var/www/html
            cat /var/www/html/index.nginx-debian.html
            sudo rm -rf /var/www/html/*
            sudo mkdir -p /var/www/html/
            ls /var/www/html
            sudo cp -r ~/lms/webapp/dist/* /var/www/html

        -> Troubleshoot / Test 

            # Database
            sudo ss -ntpl
            sudo systemctl stop postgresql
            sudo systemctl start postgresql
            sudo ss -ntpl

            # API
            pm2 status
            pm2 stop 0
            sudo ss -ntpl
            Verify with Browse - http://public-ip:8080/api
            pm2 start 0
            sudo ss -ntpl
            Verify with Browse - http://public-ip:8080/api

            # Frontend
            sudo ss -ntpl
            sudo systemctl stop nginx
            sudo ss -ntpl
            sudo systemctl start nginx
            sudo ss -ntpl

        -> IMPORTANT Terminate the Server Once LAB is Done

3rd Dec 2025
=============

    -> DNS 

        -> Domain Name Mapping 

        -> SSL Certificate Generation

            -> https://certbot.eff.org/instructions?ws=nginx&os=snap

        -> In Web Server i.e Nginx

            -> sudo vi /etc/nginx/sites-enabled/default 

                -> Update 
                    
                    server_name lms.luckycloudops.xyz;

            -> sudo nginx -t

            -> sudo systemctl restart nginx 

        -> To Generate FREE SSL visit below

            -> https://certbot.eff.org/instructions?ws=nginx&os=snap

            -> sudo snap install --classic certbot

            -> sudo ln -s /snap/bin/certbot /usr/bin/certbot

            -> sudo certbot --nginx

    -> DevOps is a set of PRACTICES that works towards the 
        AUTOMATION and INTEGRATION of the processes between 
        Software Development (Dev) and IT (Ops) teams, 
        so that organizations can BUILD, TEST, and DEPLOY softwares/applications 
        FASTER and more RELIABLE

    -> Docker == FASTER


    -> Whole LMS Application deployment took 1 Hour ( Azure Ubuntu VM ) - Diff setup

    -> Whole LMS Application deployment took 1 Hour ( Azure Redhat VM ) - Diff setup

    -> Whole LMS Application deployment took 1 Hour ( Azure SUSE VM ) - Diff setup


    -> LMS App Setup, i want this to be universal, thats what docker does 

    -> Docker Helps Build Portable Environments (1 Hour) -> 2 mins 

    -> Whole LMS Application deployment took 1 Hour ( Azure Ubuntu VM ) - Diff setup

    -> Now Using Docker Next Time Onwards, My deployment takes hardly 5 mins 


    -> Containerization 

        -> Containerization is one of the practices in devops, which provides 

            -> Lightweight and efficient way of running applications 
            
            -> Promotes faster deployments and portability for applications

        -> Applications will be 

            -> Monolithic Architecture

                -> Monolith means Large Stone 

                -> Monolithic Architecture is software design pattern, where the entire
                    application is built as single & indivisible unit 

            -> Microservices Architecture

                -> Microservices Architecture is a software development approach that involves 
                building small, independent services that work together 
                to create a larger application.

            -> Before Containerization we got on-prem or cloud(virtualization)

            -> If Applications are small / micro, why do you need 
                heavy weight VM's ?? 

                -> Answer for above problem is Containerization

    -> Docker Architecture / Components 

        -> Docker CLI
        -> Docker Daemon 
        -> Docker Registry (ACR / ECR / etc)
        -> Docker Images
        -> Docker Container 

4th Dec 2025
=============

    -> Docker Setup 

        -> In Azure B2S is the size to select. 

        -> Add Following Rules in Network Security Groups 

            -> 22, 80, 443
            -> 32768 - 61000 (docker pre defined range of ephemeral ports)
            -> 8080 - 9090 (user defined range of ephemeral ports)

            1  docker
            2  docker info
            3  sudo apt -V -s install docker.io
            4  ls
            5  curl -fsSL https://get.docker.com -o install-docker.sh
            6  ls
            7  chmod u+x install-docker.sh
            8  ./install-docker.sh
            9  docker info
            10  sudo docker info
            11  docker image ls
            12  docker container ls
            13  sudo grep docker /etc/group
            14  sudo usermod -aG docker ubuntu
            15  sudo grep docker /etc/group

            -> logout and re-login for changes to reflect 

        
    -> To Create Container We need Docker Images 

        -> Docker Images are present in Docker Registry 

            -> Docker Registry is hub.docker.com 
    
    -> Create Container 

        -> docker container run --name name_container <image> 

        -> docker container run --name c1 hello-world 

    -> Container Modes 

        -> Interactive Mode (debugging / testing mode)

            -> Interactive Mode: Containers run in the foreground, and their output is printed to the terminal.
                This mode is useful for debugging and development. Another name is Foreground Mode. 

                -> option is -it

        -> Detached Mode (production mode)

            -> Detached Mode: Containers run in the background, and their output is not printed to the terminal. 
                This mode is commonly used in production environments. Another name is Background Mode. 

                -> option is -dt
        

        -> Foreground Mode
            Create Ubuntu container in Foreground Mode:
            # Go with Two Sessions 
            docker container run --help
            docker container run -it --name c1 ubuntu:22.04


        -> Detached Mode
            Create Ubuntu container in Detached Mode:
            docker container run -dt --name c2 ubuntu:22.04

        -> Docker exec

            docker container run -dt --name c2 ubuntu:22.04
            docker container exec c2 uname
            docker container exec c2 ps -ef
            docker container exec c2 git
            docker container exec c2 ss
            docker container exec c2 ssh

            docker container stop CONTAINER_NAME
            docker container rm CONTAINER_NAME
            docker container start CONTAINER_NAME
            docker container rm -f CONTAINER_NAME

    -> Setup Web Server Container - 1

        docker container run -dt --name web ubuntu:22.04
        docker container exec -it web bash

        # Install ss command  
        ss -ntpl
        apt update -y
        apt install iproute2 -y
        ss -ntpl
        ps -ef


        # Update and Install Nginx
        apt install nginx -y

        # Start the Nginx service
        service nginx start
        ss -ntpl
        ps -ef

        # Access Nginx Web Server
        apt install curl -y
        curl localhost

    -> docker container inspect <container-name>

    -> docker stats
    

5th Dec 2025
=============

    -> Containers are non-persistent 

    -> Say we removed c1 from last class 

    -> Now again we need to start from step - 1

        -> again install nginx      
        -> again start nginx 

    
    -> Setup Web Server Container - 2

        -> While working with docker 

            -> Don't re-invent wheel again 

        -> docker container run -dt --name c2 nginx

        -> Trying to access container outside the host will not work

    
    -> Docker Networking 

        -> Docker networking enables communication

            -> between containers and the external network

            -> between containers and the host 

            -> in between containers also 


    -> Network Drivers
        
        -> Docker supports various network drivers, each designed for specific use case
            
            -> Bridge
            -> Host
            -> None

        -> Host: This driver removes network isolation between 
            the container and the Docker host, 
            allowing them to share the host’s networking namespace.

            -> docker network ls
            
            -> NOTE: By default all containers are created in Bridge network

            -> docker network inspect bridge

            -> docker network inspect host 

                -> no containers in host network

            -> Let's create container in host network Now

                -> docker container run -dt --name c3 --network=host nginx

                -> docker container ls -a

                -> docker network inspect host 

                -> docker container inspect c3

                    -> no ip for container c3, as it share network with host 

                    -> curl localhost

                    -> curl private-ip 

                    -> browse public-ip 

        -> NOTE: Challenge with host network is only one port can be used by one app 

            -> docker container run -dt --name c4 --network=host nginx

                -> Exited

            -> docker container run -dt --name c5 --network=host nginx

                -> Exited - 80 failed (98: Address already in use)

                -> docker container logs c4

    
    -> None Network

        -> This mode will not configure any IP for the container and 
            doesn’t have any access to the external network 
            as well as for other containers. 
            It does have the loopback address and can be used for running batch jobs.

           -> docker container exec c3 apt update -y (host network)

           -> docker container exec c1 apt update -y (bridge network)

           -> docker container run -dt --name c6 --network=none nginx

           -> docker container ls -a

           -> docker container inspect c6

           -> docker container exec c6 apt update -y (none network)

    -> Bridge Network

        -> The default bridge network in Docker is a pre-defined network named bridge,
         which is automatically created when you install Docker. 
         It is the default network for containers if no other network is specified.

        -> When you start a container without specifying a network, 
            it connects to the bridge network by default.

        -> Containers on the bridge network can communicate with each other using IP addresses 

            -> docker container inspect c2
                -> 172.17.0.3
            -> docker container exec -it c1 bash
            -> apt update -y && apt install curl -y
            -> curl 172.17.0.3 

        -> Bridge network supports network forwarding i.e port forwarding 

        -> Above activity makes a port(EXPOSED PORT) inside a container 
            accessible to ports on the Docker host for external access. 
            Port mapping is accomplished by binding a port on the host 
            to a port in the container, effectively forwarding traffic 
            from the host to the container. 
            This is a necessary step for any service running 
            inside a container that needs to be accessible from 
            external machines or networks.

        -> -p : docker user defined ports ( custom range 8080 to 9090 )

            -> docker container run -dt --name app1 -p 8080:80 nginx

            -> docker container run -dt --name app2 -p 8085:80 nginx

            -> docker container run -dt --name app3 -p 9090:80 nginx
        
            -> docker container ls -a

        -> -P : docker ephemeral ports ( predefined range 32768 to 61000 )

            -> docker container run -dt --name app4 -P nginx
            -> docker container run -dt --name app5 -P nginx


        -> Custom network for isolation 

            -> docker network ls
            -> docker network create --driver bridge --subnet 10.0.0.0/16 --ip-range 10.0.1.0/24 --gateway 10.0.0.1 cbn 
            -> docker network ls

            -> docker container run -dt --name cbn-1 --network=cbn ubuntu

            -> docker container ls -a

            -> docker container inspect cbn-1
                -> 10.0.1.0
            -> docker container exec -it c1 bash
            -> curl 10.0.1.0 

8th & 9th Dec 2025
==================

    -> By default containers are non-persistent in nature

    -> Docker volumes is a mechanism for persisting data generated by Docker containers.

        -> docker container run -dt --name container-name -v imp_data_vol:/path/in/container my_image

        -> docker container run -dt --name c1 -v imp_data_vol:/imp_data ubuntu:22.04

            -> Update data from container to Host 

            -> This Approach is called as Named Volumes 

        -> docker container run -dt --name c1 -v ~/imp_data:/imp_data ubuntu:22.04 

            -> Update data from Host to Container 

            -> This Approach is called as Host Volumes 


        -> Docker Named Volumes With Application

            -> docker container run -dt --name c1 -v web-app:/var/www/html -p 8080:80 nginx

            -> docker container ls -a

            -> docker container exec -it c1 bash

            -> apt update -y && apt install git -y

            -> git clone -b dev https://github.com/ravi2krishna/devops-2pm.git /var/www/html

            -> NOTE: When Browsed with port, application is not working 

                -> When working with containers nginx document root is /usr/share/nginx/html

            -> git clone -b dev https://github.com/ravi2krishna/devops-2pm.git /usr/share/nginx/html

            -> rm -r /usr/share/nginx/html

            -> git clone -b dev https://github.com/ravi2krishna/devops-2pm.git /usr/share/nginx/html

            -> exit 

            -> docker volume ls

            -> docker container ls -a

            -> docker container rm -f c1     

            -> docker container run -dt --name c1 -v web-app:/var/www/html -p 8080:80 nginx  

            -> docker container run -dt --name c2 -v web-app:/usr/share/nginx/html -p 8081:80 nginx  

            -> NOTE: Assume now application is active development mode 

                -> Updated index page 

                -> do git pull ?? 

                -> docker container exec -it c2 bash

                -> cd /usr/share/nginx/html

                -> git pull 

                -> git: command not found

            -> Now lets try from Host 

                -> docker volume inspect web-app

                -> /var/lib/docker/volumes/web-app/_data

                -> cd /var/lib/docker/volumes/web-app/_data 

                -> sudo cd /var/lib/docker/volumes/web-app/_data

            -> NOTE: Above approach causes issues with permissions, thats when we use Host Volumes 

            -> Host Volume Based Approach 

                -> git clone -b dev https://github.com/ravi2krishna/devops-2pm.git 

                -> ls 

                    -> devops-2pm
                
                -> docker container run -dt --name c3 -v ~/devops-2pm:/usr/share/nginx/html -p 8082:80 nginx  

                -> docker container ls -a

                -> Updated index page 

                -> do git pull ?? 

                    -> cd devops-2pm
                    -> git pull 

        -> All the docker volumes data is present on Host Disk, which is part of a data center 

        -> When Data center is down, our app data is down 

        -> stateless & stateful 

            -> stateless: data is not mandatory, api's, web services, websites, web apps  

            -> stateful: data is mandatory, database oriented applications 
            

    -> Dockerize / Build Docker Custom Images   

        -> Dockerizing an application is the process of converting an application 
            to run within a Docker container without dependency. 

        -> Custom Docker images allow you to tailor the software environment 
        to meet the specific requirements of your application. 
        You can install additional dependencies, configure settings, 
        and optimise the image for your use case.

        -> Docker can build images automatically by reading the instructions from a "Dockerfile"

        -> A Dockerfile is a text document that contains all the commands 
            a user could call on the command line to assemble an image.

        -> overall flow

            -> You create a Dockerfile with the required instructions in your project i.e repository 

            -> Then you will use the "docker build" command to create a Docker image 
                based on the Dockerfile that you created

            -> Then you can run the container using the image built

        -> Docker Instructions / Directives 

            FROM — set base image
            COPY — copy files to image
            RUN — execute command in container
            WORKDIR — set working directory
            VOLUME — create mount-point for a volume
            CMD — set executable for container
            EXPOSE — set port number 
            ENV — set environment variable

    -> Prepare Custom Docker Image to run devops-2pm application

        -> Create Account In Docker Hub 

        -> git clone -b dev https://github.com/ravi2krishna/devops-2pm.git 

        -> cd devops-2pm 

            -> docker build -t ravi2krishna/devops-2pm .

            -> touch Dockerfile 

                -> the Dockerfile cannot be empty
            
            -> What Instructions ??

                -> Use Docker Directives

    -> Next Goal is Dockerize Three Tier Application

        -> React + Nodejs + Postgres 

    
10th Dec 2025
=============

    -> Dockerize Three Tier Application

        -> React + Nodejs + Postgres 

    -> git clone -b dev https://github.com/ravi2krishna/lms.git

    -> sudo ss -ntpl

    -> Postgres Database - 5432

        -> docker container run -dt --name lms-db postgres 

        -> docker container ls -a

        -> docker container logs lms-db

        -> docker container rm -f lms-db

        -> docker container run -dt --name lms-db -e POSTGRES_PASSWORD=Login@123 postgres

        -> ls
            # Backend Source code
            cd ~/lms/api
            ls
            vi .env

            MODE=dev
            PORT=8080
            DATABASE_URL=postgresql://postgres:your-password@db-container-ip:5432/postgres

            # ubuntu user Session
            cat prisma/migrations/20221110085013_init/migration.sql

            -> docker build -t ravi2krishna/lms-be .
            -> docker container run -dt --name t1 ravi2krishna/lms-be
            -> docker container exec t1 node -v
            -> docker container exec t1 npm -v
            -> docker container exec t1 pwd

            -> cat Dockerfile

                # Base Image
                FROM node:16

                # RUN - runs command
                RUN mkdir /backend

                # WORKDIR - set working directroy
                WORKDIR /backend

            -> docker build -t ravi2krishna/lms-be .

            -> docker container run -dt --name t2 ravi2krishna/lms-be

            -> docker container exec t2 pwd

            -> docker container exec t1 pwd

            -> docker container exec t2 ls

            -> cat Dockerfile

                # Base Image
                FROM node:16

                # RUN - runs command
                RUN mkdir /backend

                # WORKDIR - set working directroy
                WORKDIR /backend

                # COPY 
                COPY . /backend

            -> docker build -t ravi2krishna/lms-be .

            -> docker container run -dt --name t3 ravi2krishna/lms-be

            -> docker container exec t2 ls

            -> docker container exec t3 ls

            -> docker container exec t3 cat .env

            -> cat Dockerfile 

                # Base Image
                FROM node:16

                # RUN - runs command
                RUN mkdir /backend

                # WORKDIR - set working directroy
                WORKDIR /backend

                # COPY - copy code
                COPY . /backend

                # Run Build Related Commands
                RUN npm install
                RUN npx prisma generate
                RUN npx prisma db push
                RUN npm run build

            -> docker build -t ravi2krishna/lms-be .

            -> docker container run -dt --name t4 ravi2krishna/lms-be

            -> docker container exec t4 ls build

            -> cat Dockerfile 

                # Base Image
                FROM node:16

                # RUN - runs command
                RUN mkdir /backend

                # WORKDIR - set working directroy
                WORKDIR /backend

                # COPY - copy code
                COPY . /backend

                # Run Build Related Commands
                RUN npm install
                RUN npx prisma generate
                RUN npx prisma db push
                RUN npm run build

                # Run Deployment Related Commands
                # RUN node build/index.js
                CMD ["node","build/index.js"]

            -> docker build -t ravi2krishna/lms-be .

            -> docker container run -dt --name t5 ravi2krishna/lms-be

            -> docker container inspect t5

            -> curl 172.17.0.7:8080/api

            {"message":"success","mode":"dev"}

            -> docker container run -dt --name lms-be -p 8080:8080 ravi2krishna/lms-be

        -> Verify with Browse - http://public-ip:8080/api
    
        -> ENTRYPOINT vs CMD 

            -> ENTRYPOINT

                -> Defines the main executable or command

                -> Difficult to override

                -> NOTE: Best used when you want the container to behave like fixed executable 

            -> CMD 

                -> Provides default arguments to the ENTRYPOINT instruction or, 
                    if no ENTRYPOINT is defined, serves as the default command to execute. 

                -> Easy to override

                -> NOTE: Best used when you want the container to behave with variable executable 

                    CMD["node","build/index.js"]

                    CMD["node","build/app.js"]

        
        -> Build Frontend For LMS
            
            cd ~/lms/webapp
            ls
            vi .env

            VITE_API_URL=http://public-ip:8080/api


        -> cat Dockerfile 

            # Base Image
            FROM node:16

            # RUN - runs command
            RUN mkdir /frontend

            # WORKDIR - set working directroy
            WORKDIR /frontend

            # COPY - copy code
            COPY . /frontend

            # Run Build Related Commands
            RUN npm install
            RUN npm run build

        -> docker build -t ravi2krishna/lms-fe .

        -> docker container run -dt --name t8 ravi2krishna/lms-fe

        -> docker container exec t8 ls dist

        -> cat Dockerfile 

            # Base Image
            FROM node:16

            # RUN - runs command
            RUN mkdir /frontend

            # WORKDIR - set working directroy
            WORKDIR /frontend

            # COPY - copy code
            COPY . /frontend

            # Run Build Related Commands
            RUN npm install
            RUN npm run build

            # Run Deployment On Nginx
            FROM nginx

            COPY --from=build /frontend/dist /usr/share/nginx/html

        -> docker build -t ravi2krishna/lms-fe .

        -> docker container run -dt --name t9 ravi2krishna/lms-fe

        -> docker container exec t9 ls /usr/share/nginx/html

        -> docker container run -dt --name lms-fe -p 80:80 ravi2krishna/lms-fe


11th Dec 2025
=============

    -> High Availability

        -> High Availability (HA) means designing IT systems to operate continuously 
            with minimal or zero downtime, ensuring they remain accessible and reliable 
            despite hardware/software failures by eliminating single points of failure 
            through redundancy, failover, and replication, often 
            targeting standards like "five nines" (99.999% uptime)
        
        -> High availability in AWS/AZURE ensures that applications and services are 
            continuously accessible and operational, even in the event of component failures or disruptions.

        -> High availability is achieved by deploying resources across multiple availability zones (AZs)


    -> Kubernetes 

        -> Production Grade Container Orchestration 

        -> Production-grade container orchestration refers to the capability of 
            running containerized applications in a production environment effectively and reliably.

        -> Kubernetes builds upon 15 years of experience of "running production workloads at Google"

        -> Kubernetes was originally designed by Google, and is now maintained by the 
            Cloud Native Computing Foundation (CNCF) 

    -> Kubernetes Components

    -> Master Components

        -> API Server: 
            
            -> The API server acts as the brain of the Kubernetes cluster. 
            It provides a central point of communication for all the other components. 
            When you want to create, update, or delete something in the cluster, 
            you communicate with the API server.

        -> etcd: 
            
            -> Think of etcd as the memory of the Kubernetes cluster. 
            It stores the configuration data and the state of the entire cluster. 
            When any component wants to know the current state of the cluster, it asks etcd.

        -> Scheduler: 
            
            -> The Scheduler is like a matchmaker. 
            Its job is to decide which worker node should run each new app container/pod. 
            It considers factors like available resources, to make sure app containers/pods 
            are placed on suitable worker nodes.

        -> Controller: 

            -> The Controller Manager is like the supervisor of the cluster. 
            It continuously observes the state of the cluster through the API server. 
            If it detects that the actual state of the cluster doesn't match the desired state 
            (for example, if an app container/pod goes down), 
            it takes action to make the cluster converge to the desired state.
    
    -> Worker Components

        -> Kubelet: 

            -> The Kubelet is like the worker's Agent Software.  
            It runs on every worker node and is responsible for making sure 
            that the containers/pods assigned to its node are running and healthy. 
            It takes care of starting, stopping, and monitoring containers within the pods. 
            It updates the status back to Master.

        -> Docker: 

            -> The container runtime is the software that runs and manages containers.  
            The container runtime is responsible for pulling container images from a registry, 
            creating containers, and managing their life cycles.

        -> Kube Proxy: 

            -> The Kube Proxy is like the traffic cop of the worker nodes. 
            It manages the network connections and routing between the containers/pods and services 
            within the cluster. It ensures that network traffic is correctly directed to the right destination.
    
    -> Workstation Component

        -> Kubectl: 

            -> API server is the main entrypoint to the k8s cluster, 
            so if you want to do anything in Kubernetes(k8s) cluster, 
            you first have to talk to the API server and the way to talk is through 
            a client software called KUBECTL, a CLI tool. 

    -> Setup Kubernetes (k8s)

    -> Minikube - Mini Kubernetes (Cost Saving - Learning)

    -> EKS - AKS - GKE (Production Setup - Highly Available Hardware)


    -> Minikube

        -> Minikube is a single node cluster, 
        where the master processes and worker processes and workstation all run on one node (one server).

        -> Minikube is primarily used for local development and learning purposes. 

        -> NOTE: Minikube is great for local development, 
        but it is not designed for running production workloads at scale. 

        -> For production environments, you would typically use managed Kubernetes services provided by 
        cloud providers i.e AWS EKS or AZURE AKS or GOOGLE GKE etc

        -> Minikube Setup 

            -> Guide - https://minikube.sigs.k8s.io/docs/start/

            -> In Azure B2S is the size to select, allow ports 22, 80 and [30000-32767] Traffic. 

            -> docker
            -> wget -O get-docker.sh https://get.docker.com && sh get-docker.sh
            -> sudo usermod -aG docker ubuntu
            -> logout
            -> docker
            -> minikube
            -> curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
            -> sudo install minikube-linux-amd64 /usr/local/bin/minikube
            -> minikube
            -> minikube status
            -> minikube start 
            -> minikube status
            -> kubectl
            -> curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
            -> chmod u+x kubectl 
            -> sudo mv kubectl /usr/local/bin/kubectl
            -> kubectl

    -> Kubernetes Objects 

        -> By creating an object, you’re effectively telling the Kubernetes system 
        what applications to run in the cluster

        -> Few examples of kubernetes objects include pods, replicasets, deployments, services etc. 

        -> To work with Kubernetes objects, whether to create, modify or delete them, 
        you’ll need to use a command-line tool called KUBECTL. 

        -> https://kubernetes.io/docs/concepts/overview/working-with-objects/

        -> A Kubernetes object is a "record of intent"--once you create the object, 
        the Kubernetes system will constantly work to ensure that the object exists. 
        By creating an object, you're effectively telling the Kubernetes system 
        what you want your cluster's workload to look like; this is your cluster's desired state.

        -> Object spec and status

        -> Object spec

            -> Almost every Kubernetes object includes two nested object fields 
            that govern the object's configuration: the object spec and the object status. 
            For objects that have a spec, you have to set this when you create the object, 
            providing a description of the characteristics you want the resource to have: its desired state.

        -> Object status

            -> The status describes the current state of the object, 
            supplied and updated by the Kubernetes system and its components. 
            The Kubernetes control plane continually and actively manages 
            every object's actual state to match the desired state you supplied.

        -> Manage Objects 

            -> Commands - Imperative Commands 

            -> Code - Declarative Configuration Files / Manifests (YAML) (Recommend)


12th Dec 2025
=============

    -> kubectl get pods

    -> pods are objects 

    -> A Pod represents a unit of deployment: A single instance of an application in Kubernetes. 

    -> A Pod can contain one or more containers 

    -> Pod Using Commands 

        -> kubectl run name-of-pod --image <image> --port=port_num

        -> kubectl run nginx-pod --image nginx --port=80

        -> kubectl get pods

        -> kubectl describe pod nginx-pod

        -> kubectl exec nginx-pod -- cat /usr/share/nginx/html/index.html 

        -> kubectl delete pod nginx-pod

        -> kubectl get pods

    -> Pod Using Code 

        -> Code - YAML     

        -> YAML: A human-readable data serialization language. 
            It is commonly used for configuration files and in applications 
            where data is being stored or transmitted.
        
        -> YAML focuses on User Friendliness & Easy Readability 

        -> Serialization language

            -> XML 

                -> JSON 

                    -> YAML(Kubernetes)

        -> YAML is used to store information about different "k8s objects" 

        -> Using YAML we represent

            -> Simple key values 

            -> Dictionaries i.e set of key values 

            -> Lists i.e multiple items 
        
        -> Comments are represented using # 

            -> # line will be ignored 

        -> In k8s any yaml file has 4 top level items 

            apiVersion:
            kind:
            metadata:
            spec:

        -> k8s scripts can be executed using 
    
            -> kubectl apply -f pod.yaml 
            -> kubectl get pods 
            -> kubectl exec nginx-pod -- cat /usr/share/nginx/html/index.html 

    -> Pods are ephemeral in nature (temporary)   

    -> Pods do not "heal" or repair themselves.

    -> ReplicaSets

        -> Replica Set is often used to "guarantee the availability" 
            of a specified number of pods

        -> If our application crashes(any pod dies), replicaset will 
            recreate the pod to ensure the configured pod is always running.

        -> Replicaset helps you achieve High Availability 

        -> Replicaset Issues 

            -> Replicasets don't allow UPDATES 


15th Dec 2025
=============

    -> Deployment Object

        -> A deployment provides updates for "pods" and "replica sets"

        -> Just update kind: Deployment


    -> Service Object

        -> A service is an object that acts as an endpoint, which you use to access the pods.

    -> There are different types of kubernetes services present, 
        to expose an application running on pods
            -> NodePort         -> access over internet
            -> Cluster IP       -> access within intranet (private communication)
            -> LoadBalancer     -> access over internet provided by cloud
            -> ExternalName     -> access over internet provided as DNS 

    -> NodePort Service

        -> You can contact the NodePort Service, from outside (www) the cluster, 
            by requesting <NodeIP>:<NodePort>

            -> This is similar to docker using -P (32768 - 61000)

            -> In k8s we use (30000 - 32767)

    -> Labels & Selectors

        -> Labels: Labels are key/value pairs that are attached to objects.  

        -> Selectors: Using a label selector, we can identify a set of objects.

        -> Why No IP Address for Accessing 

            -> If you use a ReplicaSet/Deployment to run your app, 
            that ReplicaSet can create and destroy Pods dynamically. 
            From one moment to the next, you don't know how many of 
            those Pods are working and healthy; you might not even know 
            what those healthy Pods are named and ip will be dynamic too.

            -> cd ~/devops-2pm/k8s
            -> kubectl apply -f pod.yaml
            -> kubectl describe pod/nginx-pod
                -> 10.244.0.7
            -> kubectl delete pod nginx-pod
            -> kubectl apply -f pod.yaml
            -> kubectl describe pod/nginx-pod
            -> kubectl delete pod nginx-pod
    
    -> NodePort Service

        -> https://kubernetes.io/docs/concepts/services-networking/service/
        
        -> You can contact the NodePort Service, from outside (www) the cluster, 
            by requesting <NodeIP>:<NodePort>

        -> Similar to what we did in docker with -P (32768 - 61000)

        -> Node port must be in the range of 30000–32767.

        -> NOTE: Whatever the label we have in the pod definition, 
                    the same should be in "matchLabels" section

        -> kubectl port-forward --address 0.0.0.0 service/nginx-svc 31224:80
        -> kubectl port-forward --address 0.0.0.0 service/nginx-svc 31224:80

        -> kubectl get svc

        -> kubectl delete svc nginx-svc


16th Dec 2025
=============

    -> HELM Charts 

        -> Helm is a package manager for Kubernetes

        -> apt is a package manager for ubuntu 

        -> npm is a package manager for javascript  

    -> Helm uses a packaging format called "Charts"

    -> Structure Of helm charts 

        -> Chart.yaml: This file contains metadata about the Chart, 
            including the name, version, description, and maintainer information.

        -> Templates: Helm Charts include a templates directory, 
            which contains Kubernetes YAML templates with placeholders 
            for values defined in "values.yaml" 
            Helm replaces these placeholders with the 
            actual values when rendering the templates.

        -> Values.yaml: This file holds customizable configuration values for the Chart. 

    -> Helm CLI
        
        -> The Helm command-line tool allows you to interact with Helm Charts. 
        You can use commands like helm install, helm uninstall, helm upgrade, 
        and helm delete to manage Helm releases.

    -> Install Helm 

        -> helm
        -> curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
        -> chmod u+x get_helm.sh
        -> ./get_helm.sh
        -> helm 

    -> helm list -a
    -> helm install <chart-name> <chart-location> -f values-nginx.yml
    -> helm uninstall chart-name 


17th Dec 2025
=============

    -> Persistent Volumes - PV 

        -> A PersistentVolume (PV) is a piece of "storage" in the cluster

        -> PersistentVolume represents storage independent of pods 

    -> PersistentVolumeClaim (PVC)

        -> A PersistentVolumeClaim is a request for storage by a pod
    
    -> Storage Class

        -> A StorageClass in Kubernetes defines the type of stoarge to use 

    -> Pod -> Volume Claim -> Persitent Volume -> Physical Storage 

    -> https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/

    
    -> Namespaces 

        -> In Kubernetes, namespaces provide a mechanism for "isolating" groups of resources 
            within the cluster

        -> Like branches in git 

        -> https://kubernetes.io/docs/tasks/administer-cluster/namespaces/#creating-a-new-namespace

        
18th Dec 2025
=============

    -> Auto Scaling 

        -> We can scale workload depending on the demand of resources 

        -> When you scale a workload, you can either increase or decrease the number of replicas

        -> There are "manual" and "automatic" ways to scale your workloads, 
            depending on your use case

        -> Manual Scaling

            -> Just change number of replicas 

    -> Scaling workloads automatically 

        -> In Kubernetes, you can automatically scale a workload using a 
            "HorizontalPodAutoscaler" (HPA)

        -> A HorizontalPodAutoscaler (HPA for short) automatically updates a workload 
        resource (such as a Deployment), with the aim of automatically scaling the 
        workload to match demand

        -> https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/

        -> Metrics Server 

            -> You need to use a cluster that has a Metrics Server deployed and configured. 
            The Kubernetes Metrics Server collects resource metrics from the kubelets in your cluster, 
            and exposes those metrics through the Kubernetes API, 
            using an APIService to add new kinds of resource that represent metric readings.

        -> kubectl get pods -n kube-system
        -> minikube addons enable metrics-server
        -> kubectl get pods -n kube-system

        
        requests:
            cpu: 200m

        -> Here 200m is 200 milli cpu, the container asks 200m cpu to be reserved for it 

        limits:
            cpu: 500m

        -> Here 500m is 500 milli cpu, the container use upto 500m cpu not more than that 

    -> kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10

    -> Kubernetes Production Setup 

        -> In production environments, it is common to use managed Kubernetes services 
        provided by cloud providers rather than running and managing your own Kubernetes clusters.

        -> High Availability and Reliability: Managed Kubernetes services are designed to 
            provide high availability and reliability.
        
        -> They typically run multiple master nodes across different availability zones, 
            ensuring that the control plane is resilient to failures.

        -> Popular examples of managed Kubernetes services include 
        Amazon Elastic Kubernetes Service (Amazon EKS) on AWS, 
        Azure Kubernetes Service (AKS) on Microsoft Azure, and 
        Google Kubernetes Engine (GKE) on Google Cloud Platform.



29th Dec 2025
==============

    -> Ingress YAML Files 

    





